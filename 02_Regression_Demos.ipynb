{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a92b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12773e51-e0fa-40ce-afea-dcb3aeb4fc9f",
   "metadata": {},
   "source": [
    "In a command-line environment:\n",
    "\n",
    "pwd (Print Working Directory): This command displays the full path of the current directory you are in. It’s useful for knowing exactly where you are in the directory structure.\n",
    "\n",
    "ls (List): This command lists the files and folders in the current directory. It shows you the contents of the directory you’re in, helping you see what’s available to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded25b1-d1fa-4ef1-8ab0-94e8bc00c2c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f03cf2-954f-4456-9a85-47a28b6fd173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eee64aa1-4d13-426a-8d6d-62881ea09f8e",
   "metadata": {},
   "source": [
    "### Simple Linear Regression:\r\n",
    "\r\n",
    "1. **Overview**: \r\n",
    "   - In this section, we will build a simple linear regression model in Python.\r\n",
    "   - The goal is to understand how the model works, evaluate its quality, and use it for predictions.\r\n",
    "\r\n",
    "2. **Simple Linear Regression**:\r\n",
    "   - Simple linear regression uses one feature to predict the target value.\r\n",
    "   - The term \"simple\" refers to the use of just one feature. When multiple features are used, it's called multiple linear regression.\r\n",
    "\r\n",
    "3. **Model Basics**:\r\n",
    "   - Simple linear regression fits a line through data points on a scatter plot.\r\n",
    "   - Example: Predicting **price** (y-axis) based on **carat weight** (x-axis).\r\n",
    "   - After fitting the regression model, you get a line that best fits the data, though it may not be a perfect fit.\r\n",
    "\r\n",
    "4. **Linear Regression Equation**:\r\n",
    "   - The equation used in simple linear regression helps describe the relationship between the feature and the target variable.\r\n",
    "   - We will visualize how the model finds the \"line of best fit.\"\r\n",
    "\r\n",
    "5. **Model Evaluation**:\r\n",
    "   - After building the regression model, we will evaluate its accuracy using statistical tests and metrics.\r\n",
    "\r\n",
    "6. **Visualizing the Model**:\r\n",
    "   - The model’s performance is often visualized with a scatter plot and the regression line.\r\n",
    "   - The line may not pass through all data points, but it should follow the overall trend.\r\n",
    "\r\n",
    "7. **Training and Testing**:\r\n",
    "   - When we build a model, we need to split the data into **training** and **testing** datasets.\r\n",
    "   - The training dataset is used to train the model, while the test dataset is used to evaluate its performance.\r\n",
    "\r\n",
    "8. **Next Steps**:\r\n",
    "   - We will continue improving the model and learn how to interpret the summary statistics to assess its effectiveness.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe0dfa-0fab-46ea-a721-86ca138989c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f70e520-6d1a-49fd-97f8-a36d93afeecc",
   "metadata": {},
   "source": [
    "### Simple Linear Regression Model:\n",
    "\n",
    "1. **Linear Relationship**:\n",
    "   - The linear regression model describes the relationship between the feature (X) and the target (Y).\n",
    "   - **Simple Linear Regression** uses just one feature to make predictions.\n",
    "\n",
    "2. **Model Equation**:\n",
    "   - The equation for simple linear regression is:  \n",
    "     $$ \\hat{y} = \\beta_0 + \\beta_1 X $$\n",
    "\n",
    "### Explanation of Each Component:\n",
    "- **$\\hat{y}$**: This is the predicted value of the dependent variable \\( y \\) based on the model.\n",
    "- **$\\beta_0$**: This is the **intercept**. It represents the value of \\( y \\) when \\( X = 0 \\). In other words, it’s where the line crosses the y-axis.\n",
    "- **$\\beta_1$**: This is the **slope** or **coefficient** for \\( X \\). It shows how much \\( y \\) is expected to change for a one-unit increase in \\( X \\).\n",
    "- **$X$**: This is the independent variable (or predictor) whose effect on \\( y \\) we want to model.\n",
    "\n",
    "---\n",
    "\n",
    "### Error in Real Life\n",
    "\n",
    "In reality, there is always some error, meaning the model may not perfectly predict the target value.  \n",
    "The equation becomes:\n",
    "\n",
    "$$\n",
    "Y = \\hat{Y} + \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ (epsilon) is the error term that accounts for the difference between the actual value ($Y$) and the predicted value ($\\hat{Y}$).\n",
    "\n",
    "### Error Term:\n",
    "- $\\epsilon$ (error) represents the difference between the actual and predicted values.\n",
    "- The goal is to minimize this error so that the predicted values ($\\hat{Y}$) are as close as possible to the actual values ($Y$).\n",
    "\n",
    "### Minimizing Error:\n",
    "In real life, it’s rare for all points to fall perfectly on the line.  \n",
    "The aim is to make the error (or residual) as small as possible, ensuring that the predicted values are close to the actual values.\n",
    "\n",
    "### Summary:\n",
    "- The top equation shows the ideal prediction using the model.\n",
    "- The equation with the error term shows what happens in reality, with some variance from the target values due to error.\n",
    "- The goal is to minimize the difference between the predicted and actual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b891b0-79ab-4919-84ed-e8e46508863e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24ad8c90-d241-4ace-b13c-5c0de53a4b37",
   "metadata": {},
   "source": [
    "### Least Squared Error and Linear Regression\n",
    "\n",
    "1. **Error and Prediction**:\n",
    "   - **Error** is the difference between the actual value of the target (\\(Y\\)) and the predicted value **(\\(\\hat{Y}\\))** from the model.\n",
    "\n",
    "2. **Least Squared Error (LSE)**:\n",
    "   - **Least squared error** is the method used to fit a linear regression model by finding the line that best represents the relationship between the feature (\\(X\\)) and target (\\(Y\\)).\n",
    "   - This method minimizes the sum of the squared differences (errors) between the actual and predicted values.\n",
    "\n",
    "3. **Why Squared Error?**:\n",
    "   - Squaring the errors makes them positive and prevents large errors in one direction from canceling out small errors in the other direction.\n",
    "   - It also simplifies the math, making it easier to solve.\n",
    "\n",
    "4. **Impact of Outliers**:\n",
    "   - A drawback of squared errors is that **outliers** (extreme values) can greatly influence the line, potentially distorting the fit.\n",
    "\n",
    "5. **Ordinary Least Squares (OLS)**:\n",
    "   - **OLS** is another name for the traditional linear regression method we're using here. It's common in the industry.\n",
    "   - Other methods exist, but they're less commonly used in general practice.\n",
    "\n",
    "6. **Visualizing Least Squared Error**:\n",
    "   - Imagine a scatter plot where the target (\\(Y\\)) values are plotted against the feature (\\(X\\)).\n",
    "   - A very basic model might predict the average of \\(Y\\) (e.g., a constant value), which gives a simple line. However, the error (the distance from actual \\(Y\\) to the predicted \\(\\hat{Y}\\)) is large.\n",
    "   - By drawing a better line based on knowledge of \\(X\\), we can reduce the error, but it still won’t be optimal.\n",
    "   - Using **linear regression** helps find the best line, minimizing the total squared error.\n",
    "\n",
    "7. **How Linear Regression Fits the Model**:\n",
    "   - Linear regression calculates the best line by adjusting the slope and intercept to minimize the sum of squared errors.\n",
    "   - For example, if the line is:\n",
    "   $$\n",
    "   \\hat{Y} = 12 + 0.4X\n",
    "   $$\n",
    "   This represents the best line with the least error, where the sum of squared errors is the smallest.\n",
    "\n",
    "8. **In Summary**:\n",
    "   - Linear regression works by minimizing the total squared error, which gives us the optimal slope and intercept for the line that best fits the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f362de09-c47c-4439-8b7e-e2a164eee1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1cb18b4-ff96-4255-a6d1-16254903ed7f",
   "metadata": {},
   "source": [
    "### Regression Libraries in Python:\n",
    "\n",
    "1. **Different Regression Implementations**:\n",
    "   - Python offers various ways to implement regression, including coding it from scratch using basic Python and **NumPy**. However, for this course, we’ll focus on two popular libraries: **Statsmodels** and **scikit-learn**.\n",
    "\n",
    "2. **Statsmodels**:\n",
    "   - **Statsmodels** is great for **inference** (understanding the relationship between variables). It’s especially useful for those familiar with tools like SAS, R, or Excel for regression.\n",
    "   - Statsmodels provides detailed outputs, such as coefficient estimates and statistical tests, which help analyze the regression.\n",
    "   - **Drawback**: It’s harder to use in production and doesn’t always include the latest algorithms for regression.\n",
    "\n",
    "3. **Scikit-learn**:\n",
    "   - **Scikit-learn** is ideal for **prediction**. It’s a popular library for machine learning in Python and offers a variety of regression algorithms.\n",
    "   - It allows easy comparison of different regression models and is well-suited for production deployment.\n",
    "   - **Drawback**: It doesn’t provide as detailed statistical outputs as Statsmodels. It focuses more on prediction and model performance.\n",
    "\n",
    "4. **Comparison Between Statsmodels and Scikit-learn**:\n",
    "   - Both libraries use the same mathematical principles and return the same regression equation.\n",
    "   - Statsmodels gives a lot of information upfront (e.g., statistical tests, coefficient estimates).\n",
    "   - Scikit-learn is simpler and focuses on prediction, but you need to explicitly ask for outputs like coefficients.\n",
    "\n",
    "5. **Regression Model Fitting**:\n",
    "   - We start by importing data processing libraries like **Pandas** and **NumPy**.\n",
    "   - Then, import **Statsmodels** (as `SM`) and **scikit-learn's** linear regression model.\n",
    "   - Data is split into features (X) and target (Y). In **Statsmodels**, we call the **OLS** function (Ordinary Least Squares), fit the model, and then use the `summary()` method to get detailed results.\n",
    "   - In **scikit-learn**, we use the `LinearRegression()` function to fit the model. It doesn’t have a `summary()` method, but you can access the coefficients directly.\n",
    "\n",
    "6. **Key Differences**:\n",
    "   - **Statsmodels** asks for the target (Y) first when fitting the model, whereas **scikit-learn** expects the features (X) first.\n",
    "   - Statsmodels automatically provides a summary of the regression model, while in scikit-learn, you must request the outputs explicitly.\n",
    "\n",
    "7. **Summary**:\n",
    "   - We’ll start with Statsmodels to explore detailed model outputs, then gradually transition to **scikit-learn** for its flexibility and suitability in prediction and production scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8172b-cf82-4274-8367-65be072cf397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bb7e91e-42de-4a1e-a1bb-153dc07e4db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ymona\\\\PythonScripts\\\\Python Data Science Regression & Forecasting\\\\Demo Notebooks'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e000dad-713f-4d45-a67e-546412aa142b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 4A1D-0E57\n",
      "\n",
      " Directory of C:\\Users\\ymona\\PythonScripts\\Python Data Science Regression & Forecasting\\Demo Notebooks\n",
      "\n",
      "11/10/2024  09:59 PM    <DIR>          .\n",
      "11/10/2024  06:11 PM    <DIR>          ..\n",
      "11/10/2024  06:11 PM    <DIR>          .ipynb_checkpoints\n",
      "11/10/2024  06:11 PM           806,443 01_EDA_Demos.ipynb\n",
      "11/10/2024  09:59 PM           111,507 02_Regression_Demos.ipynb\n",
      "11/10/2024  06:11 PM           274,967 02_Simple_Regression_Case_Study.ipynb\n",
      "11/10/2024  06:11 PM           147,298 03_multiple_regression_demos.ipynb\n",
      "11/10/2024  06:11 PM           714,819 04_Assumptions_Demos.ipynb\n",
      "11/10/2024  06:11 PM            15,630 05_Validating_Testing_Demos.ipynb\n",
      "11/10/2024  06:11 PM           105,491 06_feature_engineering_demos.ipynb\n",
      "11/10/2024  06:11 PM            49,314 07_regularized_regression_demos.ipynb\n",
      "11/10/2024  06:11 PM           486,064 08_time_series_demos.ipynb\n",
      "               9 File(s)      2,711,533 bytes\n",
      "               3 Dir(s)  934,688,546,816 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc40c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2   0.23     Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3   0.29  Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4   0.31     Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds = pd.read_csv(\"../Data/Diamonds Prices2022.csv\")\n",
    "\n",
    "diamonds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c1045f",
   "metadata": {},
   "source": [
    "### Fitting a Regression in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df79c698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -2256.3950475375823\n",
      "Coefficients: [7756.43615951]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = diamonds[[\"carat\"]]\n",
    "y = diamonds[\"price\"]\n",
    "\n",
    "lr = LinearRegression().fit(X, y)\n",
    "\n",
    "print(f\"Intercept: {lr.intercept_}\")\n",
    "print(f\"Coefficients: {lr.coef_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4933f9-caaf-48e5-b20d-4e04c06010c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68fe930-eec2-4a93-afc9-a60471adb5a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b12a047",
   "metadata": {},
   "source": [
    "### Fitting a Regression in Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "213b44b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.849</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.849</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>3.041e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 08 Aug 2023</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>11:03:41</td>     <th>  Log-Likelihood:    </th> <td>-4.7276e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 53943</td>      <th>  AIC:               </th>  <td>9.455e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 53941</td>      <th>  BIC:               </th>  <td>9.455e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>-2256.3950</td> <td>   13.055</td> <td> -172.840</td> <td> 0.000</td> <td>-2281.983</td> <td>-2230.807</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>carat</th> <td> 7756.4362</td> <td>   14.066</td> <td>  551.423</td> <td> 0.000</td> <td> 7728.866</td> <td> 7784.006</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14027.005</td> <th>  Durbin-Watson:     </th>  <td>   0.986</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>153060.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.939</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>11.036</td>   <th>  Cond. No.          </th>  <td>    3.65</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.849\n",
       "Model:                            OLS   Adj. R-squared:                  0.849\n",
       "Method:                 Least Squares   F-statistic:                 3.041e+05\n",
       "Date:                Tue, 08 Aug 2023   Prob (F-statistic):               0.00\n",
       "Time:                        11:03:41   Log-Likelihood:            -4.7276e+05\n",
       "No. Observations:               53943   AIC:                         9.455e+05\n",
       "Df Residuals:                   53941   BIC:                         9.455e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const      -2256.3950     13.055   -172.840      0.000   -2281.983   -2230.807\n",
       "carat       7756.4362     14.066    551.423      0.000    7728.866    7784.006\n",
       "==============================================================================\n",
       "Omnibus:                    14027.005   Durbin-Watson:                   0.986\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           153060.389\n",
       "Skew:                           0.939   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.036   Cond. No.                         3.65\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sm.add_constant(diamonds[\"carat\"])\n",
    "y = diamonds[\"price\"]\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0496e7a-d554-42cd-8e92-7fb25cca2405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e9948-e501-4352-8019-184d91ba36a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa7381bd-84a4-4b34-bf49-205c7b9bbc89",
   "metadata": {},
   "source": [
    "###  Building Regression Models with Statsmodels\n",
    "\n",
    "1. **Basic Setup**:\n",
    "   - To build a regression model in **Statsmodels**, we need to import the library and set up our data.\n",
    "   - **Steps**:\n",
    "     - Import `statsmodels.api` as `SM`.\n",
    "     - Create a data frame `X` for the features (predictor variables) and add a constant column.\n",
    "     - Create a data frame or series `Y` for the target variable.\n",
    "     - Use `SM.OLS()` to set up the model, passing `Y` for the target and `X` for the features, then call the `fit()` method to build the model.\n",
    "     - Store the model in a variable (e.g., `model`) and use `model.summary()` to view the model output.\n",
    "\n",
    "2. **Adding a Constant**:\n",
    "   - **Why Add a Constant?** Statsmodels assumes that the regression line runs through the origin (0,0) by default. Adding a constant allows the model to calculate a y-intercept that’s not necessarily zero.\n",
    "   - The constant is simply a column in the features data frame where every value is 1. This helps the model compute the intercept term (y-intercept).\n",
    "   - Without adding a constant, the model forces the line to go through the origin, which might not be appropriate in most cases.\n",
    "\n",
    "3. **How to Add a Constant**:\n",
    "   - You can manually add a constant column, but it's easier and more reliable to use the built-in `add_constant()` function in Statsmodels.\n",
    "\n",
    "4. **Interpreting the Model Summary**:\n",
    "   - The summary output can be overwhelming, but it’s divided into three main sections:\n",
    "     1. **Model Summary Statistics** (e.g., R-squared, F-statistic)\n",
    "     2. **Variable (Coefficient) Summary Statistics** (e.g., coefficient estimates, p-values)\n",
    "     3. **Residual (Error) Statistics** (e.g., standard error, residual sum of squares)\n",
    "   - It's common not to look at every detail of the summary but to focus on key sections relevant to model performance.\n",
    "\n",
    "5. **Common Issues**:\n",
    "   - If you don’t add a constant, the R-squared value will be labeled as “Uncentered,” indicating the model does not have an intercept.\n",
    "   - Once you add a constant, the model will provide a proper intercept term.\n",
    "\n",
    "6. **Code Example**:\n",
    "   - Import necessary libraries: `pandas`, `numpy`, and `statsmodels.api`.\n",
    "   - Prepare the data by selecting the relevant features (e.g., `carat` as `X` and `price` as `Y`).\n",
    "   - Fit the model with `SM.OLS(Y, X).fit()`.\n",
    "   - If you don't add a constant, the model will display \"Uncentered\" in the summary.\n",
    "   - Add a constant using `SM.add_constant(X)` to correct the model and include the intercept.\n",
    "\n",
    "7. **Storing the Model**:\n",
    "   - It's helpful to store the fitted model in a variable (e.g., `model = SM.OLS(Y, X).fit()`) so you can access different parts of the model output later.\n",
    "\n",
    "8. **Next Steps**:\n",
    "   - After building the model, we will dive deeper into interpreting the outputs and understanding key metrics like coefficients and statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1171d2-442b-4ce6-8437-78d399ffa530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98df2e95-d674-430a-ba7c-89c03f50ff0d",
   "metadata": {},
   "source": [
    "## Interpreting Linear Regression Coefficients\n",
    "\n",
    "### Linear Regression Overview:\n",
    "Linear regression is widely used due to its simplicity and ease of interpretation. It's a valuable tool for understanding relationships between variables. The key to interpreting the model lies in the **coefficients** (often referred to as the \"Coef\" column) in the variable summary statistics.\n",
    "\n",
    "### Model Setup:\n",
    "In this example, we are predicting the price of a diamond based on its carat weight. The regression equation can be written as:\n",
    "\n",
    "$$\n",
    "y = \\text{intercept} + (\\text{slope} \\times \\text{carat weight})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Intercept (constant)** = -2256 (the price when the carat weight is zero)\n",
    "- **Slope (carat coefficient)** = 7756 (the price increase for each additional carat)\n",
    "\n",
    "### Interpreting Coefficients:\n",
    "- **Intercept**: The intercept term suggests that a diamond with zero carat weight would cost **-2256**. While this is a theoretical value, it may not be meaningful in a real-world context (since diamonds can't have zero carat weight).\n",
    "  \n",
    "- **Slope (Carat Coefficient)**: The slope of **7756** indicates that for each additional carat, the price of the diamond increases by **7756**. This is a meaningful relationship that directly connects carat weight with price.\n",
    "\n",
    "### Caution with Causality:\n",
    "While the model suggests a relationship between carat weight and price, we must be cautious about claiming causality. This model reflects **correlation**, not causality. There could be other unaccounted factors (such as diamond cut, color, or clarity) influencing the price. Including these factors in the model could change the coefficients and provide a more complete picture.\n",
    "\n",
    "### Practical Considerations:\n",
    "- The **intercept** term may be less meaningful in practical terms, especially if it represents a scenario that doesn't occur in reality (like a diamond with zero carat weight).\n",
    "- However, the **slope (carat coefficient)** is the more important coefficient, as it describes the actual relationship between carat weight and price.\n",
    "\n",
    "### Next Steps:\n",
    "Once you've built the model, you can use it to make predictions on new data, such as predicting the price of a diamond based on its carat weight. The model will allow you to estimate how much the price increases as the carat weight increases, giving you valuable insights for pricing diamonds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb84c3-9be9-430f-824c-1faa7a183c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf15321-03ee-4b62-818c-e1cd5f28b2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "120742a1",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "844e8d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13256.47727148])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb4edd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_diamonds = pd.DataFrame({\"carat\": [0, .1, .3, .5, 1, 2, 3, 5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6216fbe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -2256.395048\n",
       "1    -1480.751432\n",
       "2       70.535800\n",
       "3     1621.823032\n",
       "4     5500.041112\n",
       "5    13256.477271\n",
       "6    21012.913431\n",
       "7    36525.785750\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(sm.add_constant(new_diamonds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9a544-68ac-4764-976e-948d9038a357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d63db72c-188a-4242-bc07-214df1b24c4a",
   "metadata": {},
   "source": [
    "### Notes on Using the Predict Method for Model Predictions\n",
    "\n",
    "1. **Purpose of the Predict Method**:\n",
    "   - The `predict` method is used to make predictions with a fitted model, whether for individual data points or entire datasets.\n",
    "   - It's especially useful when you want to make predictions for multiple data points (data frames).\n",
    "\n",
    "2. **Predicting a Single Data Point**:\n",
    "   - For a single prediction, you can pass the data directly to the `predict` method.\n",
    "   - Example:\n",
    "     - Model coefficients: intercept = -2256, carat coefficient = 7756.\n",
    "     - To predict the price of a 1.5-carat diamond:\n",
    "       - Input: `1` (constant), `1.5` (carat weight).\n",
    "       - Equation: `-2256 + (7756 × 1.5) = 9378.26`\n",
    "       - The predicted price for a 1.5-carat diamond is $9,378.26.\n",
    "\n",
    "3. **Predicting for Multiple Data Points**:\n",
    "   - You can create a data frame with multiple values (e.g., carat weights) and pass it to the `predict` method to get predictions for all of them.\n",
    "   - Example: A data frame with carat values like 0.5, 1, 1.5, 2, 2.5 will generate predictions for each.\n",
    "\n",
    "4. **Handling Constant Term**:\n",
    "   - When predicting, you need to manually add the constant (intercept) term by including a column with ones in your data.\n",
    "   - Without the constant, the model will not work correctly and will raise an error.\n",
    "\n",
    "5. **Example of Predicting with a Data Frame**:\n",
    "   - You can predict for multiple carat values (e.g., 0.5, 1, 1.5, etc.) using a data frame.\n",
    "   - After adding the constant column, passing this data frame into the `predict` method will return the predicted prices.\n",
    "\n",
    "6. **Handling Small Values**:\n",
    "   - If the carat values are very small (e.g., 0.1, 0.3 carats), the model may predict negative prices because of the intercept term.\n",
    "   - This is an indication that the model may not perform well for very small diamonds but should improve for larger ones with more data.\n",
    "\n",
    "7. **Model Accuracy – R Squared**:\n",
    "   - After making predictions, it's important to measure how well the model fits the data using accuracy metrics like R squared, which will be discussed next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c8cd9-95a3-4080-b65c-5f160b3c5a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06ba0673-bfd2-492c-b6ea-301f3e77539c",
   "metadata": {},
   "source": [
    "## R-Squared: The Coefficient of Determination\n",
    "\n",
    "Many of you have likely already heard the term **R-squared**, and that's because it's one of the most important metrics in regression. R-squared (also known as the **coefficient of determination**) is a measure that tells us how well our model is predicting the target variable compared to simply using its mean.\n",
    "\n",
    "### What Does R-Squared Tell Us?\n",
    "\n",
    "- R-squared measures how much better the model is at predicting the target compared to using just the mean. \n",
    "- If you remember from the line of best fit, we started with the mean and adjusted the slope of our line to reduce the sum of squares.\n",
    "\n",
    "The value of **R-squared** will always be between **0 and 1**:\n",
    "- **R-squared = 0**: The model does no better than predicting the mean. It's equivalent to using the mean as the prediction for all observations.\n",
    "- **R-squared = 1**: The model explains **100%** of the variance in the data—this would be a perfect fit.\n",
    "\n",
    "### How R-Squared Relates to Simple Regression\n",
    "\n",
    "In the case of **simple (single-variable) regression**, **R-squared** is equal to the square of the **correlation coefficient** between the independent variable (X) and the dependent variable (Y). \n",
    "\n",
    "In other words:\n",
    "$$\n",
    "R^2 = \\left( \\text{correlation coefficient} \\right)^2\n",
    "$$\n",
    "For example, if we calculate the correlation between **carat weight (X)** and **price (Y)** and square it, we get an **R-squared** of 0.849. This means our model explains 84.9% of the variation in price that is not explained by the mean price.\n",
    "\n",
    "### R-Squared Formula and Deeper Understanding\n",
    "\n",
    "The formula for **R-squared** is:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SSE}{SST}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **SSE** is the **Sum of Squared Errors**, which represents the squared distance between the observed values and the values predicted by our model. It reflects the variance of the data that is **not explained** by the model.\n",
    "- **SST** is the **Sum of Squared Total**, which represents the total variance between the observed values and the mean of the target variable.\n",
    "\n",
    "#### Breaking It Down:\n",
    "\n",
    "- **SSE**: This measures the error in the model. The smaller the SSE, the better the model fits the data.\n",
    "- **SST**: This measures the total variance in the target variable **Y**. If you just predicted the mean of **Y** for all data points, you would have this as the total variance.\n",
    "\n",
    "### What Happens at R-Squared = 0?\n",
    "\n",
    "- If **R-squared = 0**, it means our model is doing no better than using the mean value for every prediction. In this case, we could simply use the mean to make predictions and the model would perform just as well.\n",
    "\n",
    "### Relative Importance of R-Squared\n",
    "\n",
    "- **R-squared** should always be interpreted relative to the **data** and the **problem** at hand:\n",
    "  - In fields like **sports analytics**, an **R-squared of 0.05** might be considered excellent.\n",
    "  - In more precise fields like **physics** or **engineering**, where you're testing theoretical models, an **R-squared of 0.95** might indicate that the model or theory needs further refinement.\n",
    "\n",
    "Even a small improvement over the mean can have significant practical value depending on the field and the specific use case.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "R-squared is a key metric that tells us how well our model fits the data. However, it’s important to consider the context and use it along with other evaluation methods to get a full picture of model performance.\n",
    "\n",
    "Now, let's shift gears and look at how **hypothesis testing** fits into the bigger picture of model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ba8f3-ba33-4b0e-9042-196f49577bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a3f5938-51c5-4a91-95b8-de3d7c3573e0",
   "metadata": {},
   "source": [
    "### Hypothesis Testing in Regression\n",
    "\n",
    "1. **What is Hypothesis Testing?**:\n",
    "   - Hypothesis tests are used to help make decisions or draw conclusions based on data.\n",
    "   - For example, testing if one group performed better than another in a test.\n",
    "\n",
    "2. **Purpose in Regression**:\n",
    "   - While we don’t need to be experts in hypothesis testing to build regression models, we need to understand the tests included in the outputs.\n",
    "   - Regression models include hypothesis tests like the **F test** to determine whether the model is significantly better than using the mean of the target variable.\n",
    "\n",
    "3. **F Test**:\n",
    "   - The F test checks if the model adds value or if the results are just random noise.\n",
    "   - It’s about determining whether the model is useful or not. If the F test shows a significant result, we can conclude that the model is better than just predicting the mean.\n",
    "\n",
    "4. **Steps in a Hypothesis Test**:\n",
    "   - **State the null and alternative hypothesis**:\n",
    "     - **Null hypothesis** (H0): The model is not useful (i.e., no improvement over the mean).\n",
    "     - **Alternative hypothesis** (Ha): The model is useful (i.e., it’s better than the mean).\n",
    "   - **Set a significance level (alpha)**: This determines the threshold for making decisions. A lower alpha means it’s harder to reject the null hypothesis, while a higher alpha makes it easier.\n",
    "   - **Calculate the test statistic and p-value**: These help assess the strength of the evidence against the null hypothesis.\n",
    "   - **Draw a conclusion**:\n",
    "     - If **p-value ≤ alpha**, reject the null hypothesis, meaning the model is likely useful.\n",
    "     - If **p-value > alpha**, don’t reject the null hypothesis, meaning the model might not be useful and needs further adjustments (e.g., more data or features).\n",
    "\n",
    "5. **F Test Hypotheses**:\n",
    "   - **Null hypothesis (H0)**: F = 0 (the model is no better than the mean).\n",
    "   - **Alternative hypothesis (Ha)**: F ≠ 0 (the model is better than the mean).\n",
    "   - The goal is to reject the null hypothesis and show that the model is adding value.\n",
    "\n",
    "6. **Significance Level (Alpha)**:\n",
    "   - Alpha represents the probability threshold for error.\n",
    "   - The industry standard is often **alpha = 0.05**, meaning there's a 5% chance of making an error.\n",
    "   - Some industries, like pharmaceuticals, use stricter thresholds (e.g., **alpha = 0.01 or 0.001**) to avoid errors that could have severe consequences.\n",
    "\n",
    "7. **Conclusion**:\n",
    "   - The hypothesis test, especially the F test, helps evaluate whether a regression model is meaningful or if it’s just producing random results.\n",
    "   - The significance level helps control the risk of making incorrect decisions about the model’s effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b490afed-9f0b-4778-b9a1-772b7ab1b493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d949bae5-c4d0-4cef-9853-77dd1fe01687",
   "metadata": {},
   "source": [
    "## F-statistic and P-value: Key Concepts in Model Evaluation\n",
    "\n",
    "The **F-statistic** is an important measure in regression analysis that helps us understand the **overall significance** of the model. Specifically, it represents the ratio of the **variance explained by the model** to the **variance not explained** by the model. In simple terms, it tells us how well our model is fitting the data compared to a model that predicts only the mean.\n",
    "\n",
    "### Relationship Between R-squared and F-statistic\n",
    "\n",
    "The F-statistic is closely related to **R-squared**, as both give us an idea of how well the model explains the variability in the target variable (Y). However, while **R-squared** looks at how much of the variance is explained by the model, the **F-statistic** evaluates the **overall significance** of the regression model, considering all variables together.\n",
    "\n",
    "### What Does the F-statistic Tell Us?\n",
    "\n",
    "- The **F-statistic** tells us if the variability explained by the model is significantly larger than the variability left unexplained.\n",
    "- A **high F-statistic** indicates that the model explains a significant portion of the variability in the target variable, making it a good model overall.\n",
    "- A **low F-statistic** means the model does not explain much more variability than a model that would predict just the mean of the target variable.\n",
    "\n",
    "### P-value for the F-statistic\n",
    "\n",
    "The **P-value** associated with the F-statistic tells us the **probability** that our model is predicting poorly or is not better than a simple mean model.\n",
    "\n",
    "- If the **P-value** is **low** (typically less than **0.05**), it suggests that the model is statistically significant, and we can confidently say that the predictors (independent variables) in the model are related to the target variable.\n",
    "- If the **P-value** is **high** (greater than **0.05**), it suggests that the model does not provide enough evidence to reject the null hypothesis, meaning the predictors are not statistically significant.\n",
    "\n",
    "### Interpreting the F-statistic and P-value in Practice\n",
    "\n",
    "Let's take a look at the **model summary**:\n",
    "\n",
    "- **F-statistic**: 30,000, which is a **very high value**. This suggests that the model explains a lot of the variance in the target variable.\n",
    "- **P-value for F-statistic**: Near zero, which indicates a very **low probability** that the model is predicting poorly.\n",
    "\n",
    "#### Significance of the P-value\n",
    "\n",
    "- Since the **P-value** is near **zero** (far less than the significance threshold, typically **0.05**), we can reject the **null hypothesis**.\n",
    "- This means that the model is statistically significant and that the predictors (such as **carrot weight**) are good predictors of the target variable (**diamond price**).\n",
    "\n",
    "### Conclusion: Hypothesis Testing\n",
    "\n",
    "- If the **P-value** is **less than alpha** (0.05), we **reject the null hypothesis**, meaning the model is significantly better than using just the mean.\n",
    "- If the **P-value** is **greater than alpha**, we **fail to reject** the null hypothesis, meaning the model may not provide a significant improvement over the mean.\n",
    "\n",
    "In our case, with a **P-value near zero**, we can confidently reject the null hypothesis and conclude that **carrot weight is a good predictor** of the price of a diamond.\n",
    "\n",
    "### Summary:\n",
    "- **F-statistic**: Tells us the overall significance of the model by comparing explained vs. unexplained variance.\n",
    "- **P-value**: Helps us determine if the model is significantly better than predicting just the mean. If the P-value is small (less than 0.05), we reject the null hypothesis and conclude that the model is meaningful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8572f4-bd0a-40e6-961d-54bbb3b16bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b815e-3636-47e4-9fe2-8395b7b20cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ce42eed",
   "metadata": {},
   "source": [
    "### Residual Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d7cdf44-dbe0-4447-8d99-9b292e44066f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sns\u001b[38;5;241m.\u001b[39mscatterplot(x\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(), y\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mresid)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=model.predict(), y=model.resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a78385-8e26-4c2f-8a99-17f845a0834c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9ad69f3-f984-48dd-9620-e9e3dbe0094a",
   "metadata": {},
   "source": [
    "## Evaluating Model Fit Using Residuals\n",
    "\n",
    "In addition to metrics like $R^2$ and hypothesis tests like the **F-test**, another very useful way to evaluate how well a model fits is through **visualization**.\n",
    "\n",
    "### Visualizing Residuals:\n",
    "\n",
    "Residual plots show how well a model performs across the range of predictions. Ideally, we want to see that these residual plots:\n",
    "- Are **centered around zero**,\n",
    "- Are roughly **normally distributed**,\n",
    "- Do not have any **clear pattern**.\n",
    "\n",
    "In these plots:\n",
    "- The **x-axis** represents the predicted values of the target variable.\n",
    "- The **y-axis** represents the **residuals** or errors of the predictions.\n",
    "\n",
    "If the residual is equal to zero, that means we perfectly predicted the value, and points that fall along the zero line fall exactly on the regression line.\n",
    "\n",
    "### Understanding Residuals:\n",
    "\n",
    "For example, if the predicted value is 30 and the residual is 10, that indicates that:\n",
    "- We **under-predicted** by 10 units, meaning the actual value is 10 units higher than the predicted value.\n",
    "\n",
    "### Relation to Sum of Squared Error:\n",
    "\n",
    "This ties directly into the **sum of squared errors (SSE)**, which is the value the model is trying to minimize when fitting the regression line.\n",
    "\n",
    "By visually inspecting residuals, if we see any clear patterns, this might indicate areas where we could improve the model and further reduce the error.\n",
    "\n",
    "### Residual Plot Example:\n",
    "\n",
    "In **Statsmodels**, the `model.resid` method returns a series with the residuals, which are calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Residual} = Y - \\hat{Y}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $Y$ is the actual value,\n",
    "- $\\hat{Y}$ is the predicted value.\n",
    "\n",
    "For example:\n",
    "- Actual price = \\$666,\n",
    "- Predicted price = \\$846,\n",
    "- Residual = \\$666 - \\$846 = -180.\n",
    "\n",
    "This negative residual indicates **over-prediction**, meaning the model has predicted too high.\n",
    "\n",
    "### Interpreting the Residual Plot:\n",
    "\n",
    "When plotting all the residuals, you might observe a strange shape. Some of this is caused by the fact that diamond prices in the dataset are capped at roughly \\$20,000.\n",
    "\n",
    "- **Positive residuals** indicate **under-prediction** (the actual value is greater than the predicted value).\n",
    "- **Negative residuals** indicate **over-prediction** (the predicted value is higher than the actual value).\n",
    "\n",
    "As we observe the plot:\n",
    "- For smaller diamonds, the model performs well and the residuals are small.\n",
    "- As the price increases (for larger diamonds), the model begins to **over-predict**, especially for diamonds near the upper end of the price range. This suggests that the model isn't capturing the characteristics of these larger diamonds well.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "We'll use the residual plot as part of our **model diagnostics** later in the course. Even for simple regressions, residual plots can quickly help us visualize:\n",
    "- Where the model performs well (residuals close to zero),\n",
    "- Where the model doesn’t fit so well, which can inform decision-making.\n",
    "\n",
    "In the next section, we'll go through a **case study** that combines all of these steps to see how they work together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b2ebf-ba93-4078-a16f-051afd5615f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd7f5051-43dd-4a43-801f-1edc19ce6ead",
   "metadata": {},
   "source": [
    "In the context of linear regression, residuals are the differences between the actual values $Y$ and the predicted values $\\hat{Y}$ for each data point. Residuals are calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Residual} = Y - \\hat{Y}\n",
    "$$\n",
    "\n",
    "- **Positive residuals** ($Y - \\hat{Y} > 0$) indicate under-prediction because the actual value $Y$ is larger than the predicted value $\\hat{Y}$. In other words, the model has predicted too low for that particular observation.\n",
    "\n",
    "- **Negative residuals** ($Y - \\hat{Y} < 0$) indicate over-prediction because the actual value $Y$ is smaller than the predicted value $\\hat{Y}$. This means the model has predicted too high for that particular observation.\n",
    "\n",
    "### Intuition:\n",
    "- **Under-prediction** (positive residual) occurs when the model's prediction is too small compared to the true value, and thus the residual is positive.\n",
    "- **Over-prediction** (negative residual) occurs when the model's prediction is too large compared to the true value, and thus the residual is negative.\n",
    "\n",
    "### In summary:\n",
    "- **Positive residual**: The model underestimates the target value.\n",
    "- **Negative residual**: The model overestimates the target value.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
